---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

This is Dr Ali Shahin Shamsabadi (ashahinshamsabadi@brave.com)! I am a Privacy Researcher at [Brave Software](https://brave.com). Before joining Brave Software, I was a Research Associate at [The Alan Turing Institute](https://www.turing.ac.uk) under the supervision of [Adrian Weller](http://mlg.eng.cam.ac.uk/adrian/), and a Postdoctoral Fellow at [Vector Institute](https://vectorinstitute.ai) under the supervision of [Nicolas Papernot](https://www.papernot.fr). During my PhD, I was very fortunate to work under [Aur√©lien Bellet](http://researchers.lille.inria.fr/abellet/), [Adria Gascon](https://scholar.google.com/citations?hl=en&user=M1FQ49sAAAAJ&view_op=list_works&sortby=pubdate), [Hamed Haddadi](https://www.imperial.ac.uk/people/h.haddadi), [Matt Kusner](https://mkusner.github.io) and [Emmanuel Vincent](https://members.loria.fr/EVincent/). 

Main blog posts I co-authored: [Verifiable differentially private ML](https://brave.com/confidential-dpproof/) and [Privacy risks in Federated Learning](https://www.cleverhans.io/2022/04/17/fl-privacy.html).

My research has been cited in the press including [Thurrott](https://www.thurrott.com/a-i/298422/brave-proposes-a-framework-to-ensure-machine-learning-models-are-trained-privately) and [TuringTop10](https://www.turing.ac.uk/sites/default/files/2023-06/pioneering_new_approaches_to_verifying_the_fairness_of_ai_models_0.pdf).


## Research 
My research interests lie in identifying and mitigating the potential risks stemming from the use of AI in high-stake decision systems to unleash the full potential of AI while safeguarding our fundamental values and keeping us safe and secure. In particular, I 
1. identify failure modes for AI systems by attacking them in terms of privacy ([Mnemonist](https://openreview.net/pdf?id=oVn5GLyONY), [Trap weights](https://arxiv.org/pdf/2112.02918.pdf)), fairness ([Fairwashing](https://openreview.net/pdf?id=3vmKQUctNy)) and security/safety ([ColorFool](https://openaccess.thecvf.com/content_CVPR_2020/papers/Shamsabadi_ColorFool_Semantic_Adversarial_Colorization_CVPR_2020_paper.pdf), [Mystique](https://arxiv.org/pdf/2202.02751.pdf), [EdgeFool](https://arxiv.org/pdf/1910.12227.pdf), [FilterFool](https://arxiv.org/pdf/2008.06069.pdf) and [FoolHD](https://arxiv.org/pdf/2011.08483.pdf));
2. mitigate these emerging risks by designing secure and trustworthy (privacy-preserving, robust, fair and explainable) AI to be deployed by institutions ([Losing Less](https://petsymposium.org/popets/2023/popets-2023-0083.pdf), [QUOTIENT](https://arxiv.org/pdf/1907.03372), [DPspeech](https://arxiv.org/pdf/2202.11823.pdf), [GAP](https://arxiv.org/pdf/2203.00949.pdf), [DarkneTZ](https://arxiv.org/pdf/2004.05703) and [Private-Feature Extraction](https://arxiv.org/pdf/1802.03151.pdf) and [PrivEdge](https://arxiv.org/pdf/2004.05574)); 
3. build confidential and reliable auditing frameworks that can be used by the public to audit the trustworthiness of AI-driven services provided by institutions ([Confidential-DPproof](https://openreview.net/pdf?id=PQY2v6VtGe), [Confidential-PROFITT](https://openreview.net/forum?id=iIfDQVyuFD),  and [Zest](https://openreview.net/forum?id=OUz_9TiTv9j)). <br />

My research has been published at top-tier conferences including NeurIPS, ICLR, CVPR, CCS, USENIX Security and PETs. 

## News
- <b> [January 2024] </b> Paper accepted at the 12th International Conference on Learning Representations [ICLR2024](https://iclr.cc), called [Confidential-DPproof: Confidential Proof of Differentially Private Training](https://alishahin.github.io/). <span style="background-color:red"><font color="white"> spotlight </font></span>
- <b> [September 2023] </b> A new preprint on LLMs and privacy, called [Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey](https://arxiv.org/pdf/2310.01424.pdf).
- <b> [July 2023] </b> Recieved Best Reviewers Free Registration award from [FL@ICML](https://fl-icml2023.github.io/)!
- <b> [July 2023] </b> Our project [Confidential-PROFITT: Confidential PROof of FaIr Training of Trees](https://openreview.net/forum?id=iIfDQVyuFD) is selected as Turing's top 10 projects of 2022-2023, see [Pioneering New Approaches to Verifying the Fairness of AI Models](https://t.co/I2b4PUPkbs)!
- <b> [July 2023] </b> Presented 2 papers at [PETS2023](https://petsymposium.org/2023/): [Differentially Private Speaker Anonymization](https://drive.google.com/file/d/1s-da_nW0Rz-RJO6Rovo0qM8auvpOYTg8/view?usp=share_link) and [Losing Less: A Loss for Differentially Private Deep Learning](https://drive.google.com/file/d/1leWqbEArDvZzM_zCEtjk6i_Vk5uvTpnF/view?usp=share_link)!
- <b> [July 2023] </b> Started as a Privacy Researcher at [Brave Software](https://brave.com)!
- <b> [May 2023] </b> Paper accepted at the 39th Conference on Uncertainty in Artificial Intelligence [UAI](https://www.auai.org/uai2023/), called [Mnemonist: Locating Model Parameters that Memorize Training Examples](https://openreview.net/pdf?id=oVn5GLyONY). 
- <b> [March 2023] </b> Paper accepted at the 23rd Privacy Enhancing Technologies symposium [PETs](https://petsymposium.org/index.php), called [Losing Less: A Loss for Differentially Private Deep Learning](https://petsymposium.org/popets/2023/popets-2023-0083.pdf).
- <b> [March 2023] </b> Organising AI UK 2023 workshop, [Privacy and Fairness in AI for Health](https://private-fair-ai.github.io).
- <b> [January 2023] </b> Paper accepted at the 11th International Conference on Learning Representations [ICLR](https://iclr.cc), called [Confidential-PROFITT: Confidential PROof of FaIr Training of Trees](https://openreview.net/forum?id=iIfDQVyuFD). <span style="background-color:red"><font color="white"> notable top 5% of accepted papers </font></span>
- <b> [November 2022] </b> 2 papers accepted at the 32nd [USENIX Security Symposium](https://www.usenix.org/conference/usenixsecurity23), called [GAP: Differentially Private Graph Neural Networks with Aggregation Perturbation](https://arxiv.org/pdf/2203.00949.pdf) and [Tubes Among US: Analog Attack on Automatic Speaker Identification](https://alishahin.github.io).
- <b> [November 2022] </b> Co-organizing the Privacy Preserving Machine Learning [PPML'2022](https://ppml-workshop.github.io/ppml22/) workshop co-located with [FOCS'2022](https://focs2022.eecs.berkeley.edu).
- <b> [September 2022] </b> Paper accepted at the 36th Conference on Neural Information Processing Systems [NeurIPS](https://neurips.cc), called [Washing The Unwashable: On The (Im)possibility of Fairwashing Detection](https://openreview.net/pdf?id=3vmKQUctNy), [<font color="blue">Code</font>](https://github.com/cleverhans-lab/FRAUD-Detect).
- <b> [September 2022] </b> 2 papers accepted at the 23rd Privacy Enhancing Technologies symposium [PETs](https://petsymposium.org/index.php), called [Differentially Private Speaker Anonymization](https://petsymposium.org/popets/2023/popets-2023-0007.pdf) and [Private Multi-Winner Voting for Machine Learning](https://alishahin.github.io).
- <b> [August 2022] </b> Chair and organise [Olya Ohrimenko](https://scholar.google.com/citations?hl=en&user=lzfVm_8AAAAJ&view_op=list_works&sortby=pubdate) in-person talk at Turing AI Programme.
- <b> [July 2022] </b> Chair and organise [Nicolas Papernot](https://www.papernot.fr) in-person talk at Turing AI Programme.
- <b> [January 2022] </b> Paper accepted at the 10th International Conference on Learning Representations [ICLR](https://iclr.cc), called [A Zest of LIME: Towards Architecture-Independent Model Distances](https://openreview.net/forum?id=OUz_9TiTv9j), [<font color="blue">Code</font>](https://github.com/cleverhans-lab/Zest-Model-Distance).
- <b> [December 2021] </b> A new preprint on federated learning privacy attack, called [When the Curious Abandon Honesty: Federated Learning Is Not Private](https://arxiv.org/pdf/2112.02918.pdf).
- <b> [November 2021] </b> Started as a Research Associate at [The Alan Turing Institute](https://www.turing.ac.uk) under supervision of [Adrian Weller](http://mlg.eng.cam.ac.uk/adrian/).
- <b> [August 2021] </b> Paper accepted at IEEE Transactions on Image Processing [TIP](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83), called [Semantically Adversarial Learnable Filters](https://arxiv.org/pdf/2008.06069.pdf), [<font color="blue">Code</font>](https://github.com/AliShahin/FilterFool)!
- <b> [March 2021] </b> Successfully passed PhD viva!
- <b> [February 2021] </b> Started a Postdoctoral fellow at [Vector Institute](https://vectorinstitute.ai) under supervision of [Nicolas Papernot](https://www.papernot.fr).
- <b> [January 2021] </b> Paper accepted at 46th International Conference on Acoustics, Speech, and Signal Processing, [ICASSP2021](https://2021.ieeeicassp.org), called [FoolHD: Fooling speaker identification by Highly imperceptible adversarial Disturbances](https://arxiv.org/pdf/2011.08483.pdf), [<font color="blue">Code</font>](https://fsepteixeira.github.io/FoolHD/)! (acceptance rate 19%)
- <b> [October 2020] </b> Giving a [toturial](http://cis.eecs.qmul.ac.uk/privacymultimedia.html) at [ACM Multimedia 2020 conference](https://2020.acmmm.org), Part2: adversarial images. 
- <b> [September 2020] </b> Offered an internship at [Inria](https://www.inria.fr/en/centre-inria-lille-nord-europe), under supervision of Aurelien Bellet.
- <b> [April 2020] </b> Selected as 200 young researchers from all over the world for 8th [HEIDELBERG LAUREATE FORUM](https://www.heidelberg-laureate-forum.org/about-us.html) by international experts appointed by award-granting institutions: The Association for Computing Machinery (ACM), the Norwegian Academy of Science and Letters (DNVA) and the International Mathematical Union (IMU)!
- <b> [March 2020] </b> Paper accepted at IEEE Transactions on Information Forensics and Security [TIFS](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206), called [PrivEdge: From Local to Distributed Private Training and Prediction](https://arxiv.org/pdf/2004.05574.pdf), [<font color="blue">Code</font>](https://github.com/smartcameras/PrivEdge)! (impact factor 6.2)
- <b> [March 2020] </b> Paper accepted at IEEE Transactions on Multimedia [TMM](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046), called [Exploiting Vulnerabilities of Deep Neural Networks for Privacy Protection](https://arxiv.org/pdf/2007.09766.pdf), [<font color="blue">Code</font>](https://github.com/smartcameras/RP-FGSM)! (impact factor 5.5)
- <b> [March 2020] </b> Paper accepted at ACM International Conference on Mobile Systems, Applications, and Services [MobiSys](https://www.sigmobile.org/mobisys/2020/), called [DarkneTZ: Towards Model Privacy on the Edge using Trusted Execution Environments](https://arxiv.org/pdf/2004.05703.pdf), [<font color="blue">Code</font>](https://github.com/mofanv/darknetz)! (acceptance rate 19%)
- <b> [Feb 2020] </b> Paper accepted at Conference on Computer Vision and Pattern Recognition, [CVPR2020](http://cvpr2020.thecvf.com), called [ColorFool: Semantic Adversarial Colorization](https://arxiv.org/pdf/1911.10891.pdf), [Video](https://www.youtube.com/watch?v=fGw1ZiqOrWo), [<font color="blue">Code</font>](https://github.com/smartcameras/ColorFool)! (acceptance rate 22%)
- <b> [Jan 2020] </b> Paper accepted at 45th International Conference on Acoustics, Speech, and Signal Processing, [ICASSP2020](https://2020.ieeeicassp.org), called [EDGEFOOL: AN ADVERSARIAL IMAGE ENHANCEMENT FILTER](https://arxiv.org/pdf/1910.12227.pdf), [Video](https://www.youtube.com/watch?time_continue=16&v=jzoo5USTUSs&feature=emb_logo), [<font color="blue">Code</font>](https://github.com/smartcameras/EdgeFool)! (acceptance rate 19%)
- <b> [Jan 2020] </b> Paper accepted at IEEE Internet of Things Journal called [A Hybrid Deep Learning Architecture for Privacy-Preserving Mobile Analytics](https://arxiv.org/pdf/1703.02952.pdf)! (impact factor 9.5)
- <b> [April 2019] </b> Paper accepted at 26th ACM Conference on Computer and Communications Security, [CCS2019](https://sigsac.org/ccs/CCS2019/), called [QUOTIENT: Two-Party Secure Neural Network Training and Prediction](https://arxiv.org/pdf/1907.03372.pdf)! (acceptance rate 16%)
- <b> [Jan 2019] </b> Paper accepted at 44th International Conference on Acoustics, Speech, and Signal Processing, [ICASSP2019](https://2019.ieeeicassp.org), called [Scene privacy protection](https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/56780/Cavallaro%20Scene%20privacy%20protection%202019%20Accepted.pdf?sequence=2), [<font color="blue">Code</font>](https://github.com/smartcameras/P-FGSM)! (acceptance rate 49%)
- <b> [June 2018] </b> Offered a PhD Enrichment scheme, a 9-month placement at [The Alan Turing Institute](https://www.turing.ac.uk)!
- <b> [March 2018] </b> Offered an internship for summer 2018 at [The Alan Turing Institute](https://www.turing.ac.uk), working on project [Privacy-aware neural network classification & training](https://aticdn.s3-eu-west-1.amazonaws.com/2017/12/Internship-Project-Descriptions-2018-FINAL-WC.pdf) under supervision of Adria Gascon, Matt Kusner, Varun Kanade!

## PC services 
- 2024: PETs‚Äô2024
- 2023: NeurIPS‚Äô2023, CCS'2023, AISTATS'2023, [ICML 2023 workshop federated learning](https://fl-icml2023.github.io) 
- 2022: ICML'2022, TIFS'2022, TOPS'2022, [Explainable AI in Finance](https://sites.google.com/view/2022-workshop-explainable-ai/) 
- 2021: ICLR'2021 [Security and Safety in Machine Learning Systems](https://aisecure-workshop.github.io/aml-iclr2021/committee), ICCV'2021 [Adversarial Robustness In the Real World](https://iccv21-adv-workshop.github.io)
- 2020: ECCV'2020 [Adversarial Robustness in the Real World](https://eccv20-adv-workshop.github.io), [42nd IEEE Symposium on Security and Privacy](http://www.ieee-security.org/TC/SP2021/cfpapers.html).


## Selected Research Talks
<figure class="third">
<iframe 
  src="https://www.youtube.com/embed/uvc49vW-UUI?si=RLddB3SNoVG7PGyT" 
  frameborder="0" webkitAllowFullScreen mozallowfullscreen allowfullscreen>
</iframe>  
<iframe 
  src="https://www.youtube.com/embed/-GE8BUZzjlo?si=7btFNQyMkYN7SSwO" 
  frameborder="0" webkitAllowFullScreen mozallowfullscreen allowfullscreen>
</iframe>
<iframe src="https://www.youtube.com/embed/Eet3AJeqTEY?si=J98i0-1JG7DjB8_u" 
  frameborder="0" webkitAllowFullScreen mozallowfullscreen allowfullscreen>
</iframe>
<iframe src="https://www.youtube.com/embed/fGw1ZiqOrWo?si=pTeOkyX3vPpuGr1F" 
  frameborder="0" webkitAllowFullScreen mozallowfullscreen allowfullscreen>
</iframe>
<iframe src="https://www.youtube.com/embed/jzoo5USTUSs?si=f1NjfhT9xxyMb_t5" 
  frameborder="0" webkitAllowFullScreen mozallowfullscreen allowfullscreen>
</iframe>  
</figure>

## Talks
- 07/2023 - UAI 2023 conference -- Mnemonist: Locating Model Parameters that Memorize Training Examples <i class="fa fa-youtube-play" style="color:blue"></i>[<font color="blue">Video</font>](https://www.youtube.com/watch?v=-GE8BUZzjlo)
- 06/2023 - PETS 2023 conference -- Losing Less: A Loss for Differentially Private Deep Learning <i class="fa fa-file-powerpoint-o" style="color:orange"></i>[<font color="orange">Slides</font>](https://drive.google.com/file/d/1leWqbEArDvZzM_zCEtjk6i_Vk5uvTpnF/view) <i class="fa fa-youtube-play" style="color:blue"></i>[<font color="blue">Video</font>](https://www.youtube.com/watch?v=Eet3AJeqTEY)
- 06/2023 - PETS 2023 conference -- Differentially Private Speaker Anonymization <i class="fa fa-file-powerpoint-o" style="color:orange"></i>[<font color="orange">Slides</font>](https://drive.google.com/file/d/1s-da_nW0Rz-RJO6Rovo0qM8auvpOYTg8/view) <i class="fa fa-youtube-play" style="color:blue"></i>[<font color="blue">Video</font>](https://www.youtube.com/watch?v=uvc49vW-UUI)
- 05/2023 - ICLR 2023 conference -- Confidential-PROFITT: Confidential PROof of FaIr Training of Trees <i class="fa fa-youtube-play" style="color:blue"></i>[<font color="blue">Video</font>](https://iclr.cc/virtual/2023/poster/11224)
- 05/2023 - [Workshop on Algorithmic Audits of Algorithms](https://algorithmic-audits.github.io) 
- 05/2023 - Intel
- 04/2023 - Northwestern University -- How can we audit Fairness of AI-driven services provided by companies?
- 03/2023 - AIUK 2023 -- Confidential-PROFITT: Confidential PROof of FaIr Training of Trees <i class="fa fa-youtube-play" style="color:blue"></i>[<font color="blue">Video</font>](https://www.youtube.com/watch?app=desktop&v=np6-cSckPV4&t=2364s)
- 03/2023 - University of Cambridge -- An Overview of Differential Privacy, Membership Inference Attacks, and Federated Learning 
- 11/2022 - NeurIPS 2022 conference -- Washing The Unwashable : On The (Im)possibility of Fairwashing Detection <i class="fa fa-youtube-play" style="color:blue"></i>[<font color="blue">Video</font>](https://neurips.cc/virtual/2022/poster/54741)
- 11/2022 - University of Cambridge and Samsung  
- 10/2022 - Queen's University of Belfast  
- 09/2022 - Information Commissioner‚Äôs Office  
- 09/2022 - Brave
- 06/2020 - CVPR 2020 conference -- ColorFool: Semantic Adversarial Colorization <i class="fa fa-youtube-play" style="color:blue"></i>[<font color="blue">Video</font>](https://www.youtube.com/watch?v=fGw1ZiqOrWo)
- 05/2020 - ACM Multimedia 2020 -- A tutorial on Deep Learning for Privacy in Multimedia <i class="fa fa-file-powerpoint-o" style="color:orange"></i>[<font color="orange">Slides</font>](https://cis.eecs.qmul.ac.uk/pdfs/2020.10.12__DeepLearningForPrivacyInMultimedia_Part2.pdf)
- 05/2020 - ICASSP 2020 conference -- EdgeFool: An Adversarial Image Enhancement Filter <i class="fa fa-youtube-play" style="color:blue"></i>[<font color="blue">Video</font>](https://www.youtube.com/watch?v=jzoo5USTUSs&t=1s)
- 06/2018 - The Alan Turing Institute -- Privacy-Aware Neural Network Classification & Training -- <i class="fa fa-youtube-play" style="color:blue"></i>[<font color="blue">Video</font>](https://www.youtube.com/watch?v=pJtw6IRo9q4)
- 06/2018 - QMUL summer school -- Distribute One-Class Learning <i class="fa fa-youtube-play" style="color:blue"></i>[<font color="blue">Video</font>](https://www.youtube.com/watch?v=9w_TP8iwpxI)
