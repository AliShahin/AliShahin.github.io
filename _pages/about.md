---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<p>
  This is Dr Ali Shahin Shamsabadi (<a href="mailto:ashahinshamsabadi@brave.com">ashahinshamsabadi@brave.com</a>)! I am a Senior Research Scientist (now expanding into product strategy and cross-functional leadership) at <a href="https://brave.com">Brave Software</a>. I collaborate across disciplines and organizations to turn scientific insights into innovative, impactful products. Before joining Brave Software, I was a Research Scientist at 
  <a href="https://www.turing.ac.uk">The Alan Turing Institute</a> (Safe and Ethical AI) under the supervision of 
  <a href="http://mlg.eng.cam.ac.uk/adrian/">Adrian Weller</a>, and a Postdoctoral Fellow at 
  <a href="https://vectorinstitute.ai">Vector Institute</a> under the supervision of 
  <a href="https://www.papernot.fr">Nicolas Papernot</a>. During my PhD, I was very fortunate to work under 
  <a href="http://researchers.lille.inria.fr/abellet/">Aur√©lien Bellet</a>, 
  <a href="https://scholar.google.com/citations?hl=en&user=KZmcljoAAAAJ&view_op=list_works&sortby=pubdate">Andrea Cavallaro</a>, 
  <a href="https://scholar.google.com/citations?hl=en&user=M1FQ49sAAAAJ&view_op=list_works&sortby=pubdate">Adria Gascon</a>, 
  <a href="https://www.imperial.ac.uk/people/h.haddadi">Hamed Haddadi</a>, 
  <a href="https://mkusner.github.io">Matt Kusner</a> and 
  <a href="https://members.loria.fr/EVincent/">Emmanuel Vincent</a>.
</p>
<p>
  My research has been cited in the press including 
  <a href="https://www.thurrott.com/a-i/298422/brave-proposes-a-framework-to-ensure-machine-learning-models-are-trained-privately">
    Thurrott
  </a> and 
  <a href="https://www.turing.ac.uk/sites/default/files/2023-06/pioneering_new_approaches_to_verifying_the_fairness_of_ai_models_0.pdf">
    TuringTop10
  </a>.
</p>

<div class="research-section">
<h2>Blog Posts</h2>

<ul>
  <li><a href="https://brave.com/blog/privacy-in-llms/">Membership Privacy Risks in LLMs</a></li>
  <li><a href="https://brave.com/blog/nebula/">Differentially private data collection</a></li>
  <li><a href="https://brave.com/confidential-dpproof/">Verifiable differentially private ML</a></li>
  <li><a href="https://medium.com/@ashahin.ee/zero-knowledge-proofs-made-easy-understanding-and-applying-zero-knowledge-proof-in-machine-ccc3a41853c2">Applying Zero Knowledge Proof in ML</a></li>
  <li><a href="https://www.cleverhans.io/2022/04/17/fl-privacy.html">Privacy risks in Federated Learning</a></li>
</ul>

</div>




<div class="research-section">
<h2>Research</h2>

My research initiates a fundamental question: 
<strong> How can we reliably verify the trustworthiness of AI-based services, given that: i) AI-based services are provided as "black-boxes" to protect intellectual property; ii) Institutions are materially disincentivized from trustworthy behavior. </strong>

<h3>Verifiable Trustworthiness of AI in Practice</h3>

<div class="publication-list">
<ul>
<li>Verifiable alignment: <a href="https://alishahin.github.io/">SURE: SecUrely REpairs failures flagged by users</a> [NeurIPS'2025]</li>
<li>Verifiable Uncertainty: <a href="https://arxiv.org/pdf/2505.23968">Confidential Guardian</a> [ICML'2025]</li>
<li>Verifiable Privacy: <a href="https://openreview.net/pdf?id=PQY2v6VtGe">Confidential-DPproof</a> [ICLR'2024]</li>
<li>Verifiable Fairness: <a href="https://openreview.net/forum?id=iIfDQVyuFD">Confidential-PROFITT</a> [ICLR'2023], <a href="https://arxiv.org/pdf/2410.02777">OATH</a> [NeurIPS'2025]</li>
<li><a href="https://openreview.net/forum?id=OUz_9TiTv9j">Architecture-Independent Model Distances</a> [ICLR'2022]</li>
</ul>
</div>

<h3>Identifying failure modes for AI systems</h3>

<div class="publication-list">
<ul>
  <li>Privacy Attacks: <a href="https://arxiv.org/pdf/2409.13745">Context-Aware MIAs against Pre-trained LLMs</a> [EMNLP'2025], <a href="https://arxiv.org/pdf/2508.07054">Membership and Memorization in LLM Knowledge Distillation</a> [EMNLP'2025], <a href="https://openreview.net/pdf?id=oVn5GLyONY">Locating Model Parameters that Memorize Training Examples</a> [UAI'2023], <a href="https://arxiv.org/pdf/2112.02918.pdf">Trap weights</a> [Euro S&amp;P'2023]</li>
  <li>Fairness Attacks: <a href="https://openreview.net/pdf?id=3vmKQUctNy">Fairwashing</a> [NeurIPS'2022]</li>
  <li>Robustness Attacks: <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Shamsabadi_ColorFool_Semantic_Adversarial_Colorization_CVPR_2020_paper.pdf">ColorFool</a> [CVPR'2020], <a href="https://arxiv.org/pdf/2202.02751.pdf">Mystique</a> [USENIX'2022], <a href="https://arxiv.org/pdf/1910.12227.pdf">EdgeFool</a> [ICASSP'2020], <a href="https://arxiv.org/pdf/2008.06069.pdf">FilterFool</a> [TIP'2022], and <a href="https://arxiv.org/pdf/2011.08483.pdf">FoolHD</a> [ICASSP'2021]</li>
</ul>
</div>

<h3>Secure and privacy-preserving (by design) AI</h3>

<div class="publication-list">
<ul>
  <li>Privacy-preserving:  
    <a href="https://petsymposium.org/popets/2023/popets-2023-0083.pdf">A Loss for Differentially Private Deep Learning</a> [PETS'2023], 
    <a href="https://arxiv.org/pdf/2202.11823.pdf">Differentially Private Speaker Anonymization</a> [PETS'2023], 
    <a href="https://arxiv.org/pdf/2203.00949.pdf">Differentially Private Graph Neural Networks</a> [USENIX'2022],
    <a href="https://arxiv.org/pdf/1802.03151.pdf">Deep Private-Feature Extraction</a>
  </li>
  <li>Secure:  
    <a href="https://arxiv.org/pdf/1907.03372">Two-Party Secure Neural Network Training and Prediction</a> [CCS'2019], 
    <a href="https://arxiv.org/pdf/2004.05703">Model Privacy at the Edge using Trusted Execution Environments</a> [MobiSys'2020],  
    <a href="https://arxiv.org/pdf/2004.05574">From Local to Distributed Private Training and Prediction</a> [TIFS'2020]
  </li>
</ul>
</div>

</div>


<div class="research-section">
<h2>Product</h2>

<h3>Privacy Preserving Product Analytics</h3>
<p><a href="https://brave.com/blog/nebula/">Nebula</a>: a novel, practical and best-in-class system for product usage analytics with differential privacy guarantees! Nebula puts users first in product analytics: i) Formal Differential Privacy Protection; ii) Auditability, Verifiability, and Transparency; and iii) Efficiency with Minimal Impact.</p>

<h3>Privacy-Preserving Conversation Analytics</h3>
<p>Coming soon.</p>

<h3>Secure, Privacy-Preserving and Efficient Agents</h3>
<p>Coming soon.</p>

</div> 


<div class="research-section">
<h2>Recent Students</h2>

<div class="publication-list">
<ul>
<li><a href="https://jrohsc.github.io/">Jaechul Roh</a>: Brave PhD Intern (Summer 2025), Project: Privacy of web agents</li>
<li><a href="https://dzungvpham.github.io/">Dzung V. Pham</a>: Brave PhD Intern (Summer 2025), Project: Efficiency of web agents</li>
<li><a href="https://www.alishaukani.com/">Alisha Ukani</a>: Brave PhD Intern (Summer 2025), Project: Intersection of Browser privacy and Web agents</li>
<li><a href="https://saiid.ch/">Saiid El Hajj Chehade</a>: Brave PhD Visitor (Summer 2025), Project: Optimizing web presentation for agents</li>
<li><a href="https://www.comp.nus.edu.sg/~hongyan/">Hongyan Chang</a>: Brave PhD Intern (Summer 2024), Project: <a href="https://arxiv.org/pdf/2409.13745">Context-Aware Membership Inference Attacks against Pre-trained Large Language Models</a></li>
<li><a href="https://scholar.google.com/citations?hl=en&user=V0918CIAAAAJ&view_op=list_works&sortby=pubdate">Olive Franzese</a>: Brave PhD Intern (Summer 2024), Project: <a href="https://arxiv.org/pdf/2410.02777">OATH: Efficient and Flexible Zero-Knowledge Proofs of End-to-End ML Fairness</a></li>
<li><a href="https://v-smith.github.io/">Victoria Smith</a>: Alan Turing Institute PhD Enrichment Student (Fall 2022 - Fall 2023), Project: <a href="https://arxiv.org/pdf/2310.01424.pdf">Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey</a></li>
</ul>
</div>

</div>


## News
- <b> [September 2025] </b> Two papers accepted at The Thirty-Ninth Annual Conference on Neural Information Processing Systems [NeurIPS 2025](https://neurips.cc/), called [Pin the Tail on the Model: Blindfolded Repair of User-Flagged Failures in Text-to-Image Services](https://alishahin.github.io/), and [Secure and Confidential Certificates of Online Fairness](https://alishahin.github.io/).
- <b> [August 2025] </b> Two papers accepted at the conference on Empirical Methods in Natural Language Processing [EMNLP 2025](https://2025.emnlp.org/), called [Membership and Memorization in LLM Knowledge Distillation](https://arxiv.org/abs/2508.07054), and [Context-Aware Membership Inference Attacks against Pre-trained Large Language Models](https://arxiv.org/abs/2409.13745).
- <b> [May 2025] </b> Paper accepted at the 32nd ACM Conference on Computer and Communications Security [CCS2025](https://www.sigsac.org/ccs/CCS2025/), called [Nebula: Efficient, Private and Accurate Histogram Estimation](https://arxiv.org/pdf/2409.09676).
- <b> [April 2025] </b> Paper accepted at the 42nd International Conference on Machine Learning [ICML2025](https://icml.cc/), called [Confidential Guardian: Cryptographically Prohibiting the Abuse of Model Abstention](https://arxiv.org/pdf/2505.23968).
- <b> [November 2024] </b> Gave a guest lecture at Imperial College London: [Collecting Speech and Telemetry Data Privately](https://drive.google.com/file/d/1mgXaNLjRpWKS7KQioawO4xiBG_iqYgmE/view?usp=sharing).
- <b> [October 2024] </b> Gave a research talk at Google TechTalk: [Beyond Trust:
Proving Fairness and Privacy in Machine Learning](https://drive.google.com/file/d/1i8MSiDQ6Uvyvx0H56fqaNf0kNefERfSr/view?usp=sharing).
- <b> [September 2024] </b> A new preprint on LLMs and privacy, called [Context-Aware Membership Inference Attacks
against Pre-trained Large Language Models](https://arxiv.org/pdf/2409.13745).
- <b> [September 2024] </b> My first-ever privacy-preserving product, [Differentially private data collection](https://brave.com/blog/nebula/).
- <b> [September 2024] </b> A new preprint on differential privacy and telemetry data, called [Nebula: Efficient, Private and Accurate Histogram Estimation](https://arxiv.org/pdf/2409.09676).
- <b> [January 2024] </b> Paper accepted at the 12th International Conference on Learning Representations [ICLR2024](https://iclr.cc), called [Confidential-DPproof: Confidential Proof of Differentially Private Training](https://hal.science/hal-04610635/document). <span style="background-color:red"><font color="white"> spotlight </font></span>
- <b> [September 2023] </b> A new preprint on LLMs and privacy, called [Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey](https://arxiv.org/pdf/2310.01424.pdf).
- <b> [July 2023] </b> Recieved Best Reviewers Free Registration award from [FL@ICML](https://fl-icml2023.github.io/)!
- <b> [July 2023] </b> Our project [Confidential-PROFITT: Confidential PROof of FaIr Training of Trees](https://openreview.net/forum?id=iIfDQVyuFD) is selected as Turing's top 10 projects of 2022-2023, see [Pioneering New Approaches to Verifying the Fairness of AI Models](https://t.co/I2b4PUPkbs)!
- <b> [July 2023] </b> Presented 2 papers at [PETS2023](https://petsymposium.org/2023/): [Differentially Private Speaker Anonymization](https://drive.google.com/file/d/1s-da_nW0Rz-RJO6Rovo0qM8auvpOYTg8/view?usp=share_link) and [Losing Less: A Loss for Differentially Private Deep Learning](https://drive.google.com/file/d/1leWqbEArDvZzM_zCEtjk6i_Vk5uvTpnF/view?usp=share_link)!
- <b> [July 2023] </b> Started as a Privacy Researcher at [Brave Software](https://brave.com)!
- <b> [May 2023] </b> Paper accepted at the 39th Conference on Uncertainty in Artificial Intelligence [UAI](https://www.auai.org/uai2023/), called [Mnemonist: Locating Model Parameters that Memorize Training Examples](https://openreview.net/pdf?id=oVn5GLyONY). 
- <b> [March 2023] </b> Paper accepted at the 23rd Privacy Enhancing Technologies symposium [PETs](https://petsymposium.org/index.php), called [Losing Less: A Loss for Differentially Private Deep Learning](https://petsymposium.org/popets/2023/popets-2023-0083.pdf).
- <b> [March 2023] </b> Organising AI UK 2023 workshop, [Privacy and Fairness in AI for Health](https://private-fair-ai.github.io).
- <b> [January 2023] </b> Paper accepted at the 11th International Conference on Learning Representations [ICLR](https://iclr.cc), called [Confidential-PROFITT: Confidential PROof of FaIr Training of Trees](https://openreview.net/forum?id=iIfDQVyuFD). <span style="background-color:red"><font color="white"> notable top 5% of accepted papers </font></span>
- <b> [November 2022] </b> 2 papers accepted at the 32nd [USENIX Security Symposium](https://www.usenix.org/conference/usenixsecurity23), called [GAP: Differentially Private Graph Neural Networks with Aggregation Perturbation](https://arxiv.org/pdf/2203.00949.pdf) and [Tubes Among US: Analog Attack on Automatic Speaker Identification](https://alishahin.github.io).
- <b> [November 2022] </b> Co-organizing the Privacy Preserving Machine Learning [PPML'2022](https://ppml-workshop.github.io/ppml22/) workshop co-located with [FOCS'2022](https://focs2022.eecs.berkeley.edu).
- <b> [September 2022] </b> Paper accepted at the 36th Conference on Neural Information Processing Systems [NeurIPS](https://neurips.cc), called [Washing The Unwashable: On The (Im)possibility of Fairwashing Detection](https://openreview.net/pdf?id=3vmKQUctNy), [<font color="blue">Code</font>](https://github.com/cleverhans-lab/FRAUD-Detect).
- <b> [September 2022] </b> 2 papers accepted at the 23rd Privacy Enhancing Technologies symposium [PETs](https://petsymposium.org/index.php), called [Differentially Private Speaker Anonymization](https://petsymposium.org/popets/2023/popets-2023-0007.pdf) and [Private Multi-Winner Voting for Machine Learning](https://alishahin.github.io).
- <b> [August 2022] </b> Chair and organise [Olya Ohrimenko](https://scholar.google.com/citations?hl=en&user=lzfVm_8AAAAJ&view_op=list_works&sortby=pubdate) in-person talk at Turing AI Programme.
- <b> [July 2022] </b> Chair and organise [Nicolas Papernot](https://www.papernot.fr) in-person talk at Turing AI Programme.
- <b> [January 2022] </b> Paper accepted at the 10th International Conference on Learning Representations [ICLR](https://iclr.cc), called [A Zest of LIME: Towards Architecture-Independent Model Distances](https://openreview.net/forum?id=OUz_9TiTv9j), [<font color="blue">Code</font>](https://github.com/cleverhans-lab/Zest-Model-Distance).
- <b> [December 2021] </b> A new preprint on federated learning privacy attack, called [When the Curious Abandon Honesty: Federated Learning Is Not Private](https://arxiv.org/pdf/2112.02918.pdf).
- <b> [November 2021] </b> Started as a Research Associate at [The Alan Turing Institute](https://www.turing.ac.uk) under supervision of [Adrian Weller](http://mlg.eng.cam.ac.uk/adrian/).
- <b> [August 2021] </b> Paper accepted at IEEE Transactions on Image Processing [TIP](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83), called [Semantically Adversarial Learnable Filters](https://arxiv.org/pdf/2008.06069.pdf), [<font color="blue">Code</font>](https://github.com/AliShahin/FilterFool)!
- <b> [March 2021] </b> Successfully passed PhD viva!
- <b> [February 2021] </b> Started a Postdoctoral fellow at [Vector Institute](https://vectorinstitute.ai) under supervision of [Nicolas Papernot](https://www.papernot.fr).
- <b> [January 2021] </b> Paper accepted at 46th International Conference on Acoustics, Speech, and Signal Processing, [ICASSP2021](https://2021.ieeeicassp.org), called [FoolHD: Fooling speaker identification by Highly imperceptible adversarial Disturbances](https://arxiv.org/pdf/2011.08483.pdf), [<font color="blue">Code</font>](https://fsepteixeira.github.io/FoolHD/)! (acceptance rate 19%)
- <b> [October 2020] </b> Giving a [toturial](http://cis.eecs.qmul.ac.uk/privacymultimedia.html) at [ACM Multimedia 2020 conference](https://2020.acmmm.org), Part2: adversarial images. 
- <b> [September 2020] </b> Offered an internship at [Inria](https://www.inria.fr/en/centre-inria-lille-nord-europe), under supervision of Aurelien Bellet.
- <b> [April 2020] </b> Selected as 200 young researchers from all over the world for 8th [HEIDELBERG LAUREATE FORUM](https://www.heidelberg-laureate-forum.org/about-us.html) by international experts appointed by award-granting institutions: The Association for Computing Machinery (ACM), the Norwegian Academy of Science and Letters (DNVA) and the International Mathematical Union (IMU)!
- <b> [March 2020] </b> Paper accepted at IEEE Transactions on Information Forensics and Security [TIFS](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206), called [PrivEdge: From Local to Distributed Private Training and Prediction](https://arxiv.org/pdf/2004.05574.pdf), [<font color="blue">Code</font>](https://github.com/smartcameras/PrivEdge)! (impact factor 6.2)
- <b> [March 2020] </b> Paper accepted at IEEE Transactions on Multimedia [TMM](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046), called [Exploiting Vulnerabilities of Deep Neural Networks for Privacy Protection](https://arxiv.org/pdf/2007.09766.pdf), [<font color="blue">Code</font>](https://github.com/smartcameras/RP-FGSM)! (impact factor 5.5)
- <b> [March 2020] </b> Paper accepted at ACM International Conference on Mobile Systems, Applications, and Services [MobiSys](https://www.sigmobile.org/mobisys/2020/), called [DarkneTZ: Towards Model Privacy on the Edge using Trusted Execution Environments](https://arxiv.org/pdf/2004.05703.pdf), [<font color="blue">Code</font>](https://github.com/mofanv/darknetz)! (acceptance rate 19%)
- <b> [Feb 2020] </b> Paper accepted at Conference on Computer Vision and Pattern Recognition, [CVPR2020](http://cvpr2020.thecvf.com), called [ColorFool: Semantic Adversarial Colorization](https://arxiv.org/pdf/1911.10891.pdf), [Video](https://www.youtube.com/watch?v=fGw1ZiqOrWo), [<font color="blue">Code</font>](https://github.com/smartcameras/ColorFool)! (acceptance rate 22%)
- <b> [Jan 2020] </b> Paper accepted at 45th International Conference on Acoustics, Speech, and Signal Processing, [ICASSP2020](https://2020.ieeeicassp.org), called [EDGEFOOL: AN ADVERSARIAL IMAGE ENHANCEMENT FILTER](https://arxiv.org/pdf/1910.12227.pdf), [Video](https://www.youtube.com/watch?time_continue=16&v=jzoo5USTUSs&feature=emb_logo), [<font color="blue">Code</font>](https://github.com/smartcameras/EdgeFool)! (acceptance rate 19%)
- <b> [Jan 2020] </b> Paper accepted at IEEE Internet of Things Journal called [A Hybrid Deep Learning Architecture for Privacy-Preserving Mobile Analytics](https://arxiv.org/pdf/1703.02952.pdf)! (impact factor 9.5)
- <b> [April 2019] </b> Paper accepted at 26th ACM Conference on Computer and Communications Security, [CCS2019](https://sigsac.org/ccs/CCS2019/), called [QUOTIENT: Two-Party Secure Neural Network Training and Prediction](https://arxiv.org/pdf/1907.03372.pdf)! (acceptance rate 16%)
- <b> [Jan 2019] </b> Paper accepted at 44th International Conference on Acoustics, Speech, and Signal Processing, [ICASSP2019](https://2019.ieeeicassp.org), called [Scene privacy protection](https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/56780/Cavallaro%20Scene%20privacy%20protection%202019%20Accepted.pdf?sequence=2), [<font color="blue">Code</font>](https://github.com/smartcameras/P-FGSM)! (acceptance rate 49%)
- <b> [June 2018] </b> Offered a PhD Enrichment scheme, a 9-month placement at [The Alan Turing Institute](https://www.turing.ac.uk)!
- <b> [March 2018] </b> Offered an internship for summer 2018 at [The Alan Turing Institute](https://www.turing.ac.uk), working on project [Privacy-aware neural network classification & training](https://aticdn.s3-eu-west-1.amazonaws.com/2017/12/Internship-Project-Descriptions-2018-FINAL-WC.pdf) under supervision of Adria Gascon, Matt Kusner, Varun Kanade!

## PC services 
- 2024: PETs'2024, ICLR'2024, [ICLR Private ML workshop](https://pml-workshop.github.io/iclr24/)
- 2023: NeurIPS'2023, CCS'2023, AISTATS'2023, [ICML 2023 workshop federated learning](https://fl-icml2023.github.io) 
- 2022: ICML'2022, TIFS'2022, TOPS'2022, [Explainable AI in Finance](https://sites.google.com/view/2022-workshop-explainable-ai/) 
- 2021: ICLR'2021 [Security and Safety in Machine Learning Systems](https://aisecure-workshop.github.io/aml-iclr2021/committee), ICCV'2021 [Adversarial Robustness In the Real World](https://iccv21-adv-workshop.github.io)
- 2020: ECCV'2020 [Adversarial Robustness in the Real World](https://eccv20-adv-workshop.github.io), [42nd IEEE Symposium on Security and Privacy](http://www.ieee-security.org/TC/SP2021/cfpapers.html).


## Selected Research Talks

<table style="border-collapse:collapse; border:none;">
  <tr>
    <td style="border:none;">
      <iframe width="320" height="180" src="https://www.youtube.com/embed/uvc49vW-UUI?si=RLddB3SNoVG7PGyT" frameborder="0" allowfullscreen></iframe>
      <div style='text-align:center; font-weight:bold;'>Differentially Private Speaker Anonymization (PETS 2023)</div>
    </td>
    <td style="border:none;">
      <iframe width="320" height="180" src="https://www.youtube.com/embed/-GE8BUZzjlo?si=7btFNQyMkYN7SSwO" frameborder="0" allowfullscreen></iframe>
      <div style='text-align:center; font-weight:bold;'>Mnemonist: Locating Model Parameters (UAI 2023)</div>
    </td>
    <td style="border:none;">
      <iframe width="320" height="180" src="https://www.youtube.com/embed/Eet3AJeqTEY?si=J98i0-1JG7DjB8_u" frameborder="0" allowfullscreen></iframe>
      <div style='text-align:center; font-weight:bold;'>Losing Less: A Loss for DP Deep Learning (PETS 2023)</div>
    </td>
  </tr>
  <tr>
    <td style="border:none;">
      <iframe width="320" height="180" src="https://www.youtube.com/embed/fGw1ZiqOrWo?si=pTeOkyX3vPpuGr1F" frameborder="0" allowfullscreen></iframe>
      <div style='text-align:center; font-weight:bold;'>ColorFool: Semantic Adversarial Colorization (CVPR 2020)</div>
    </td>
    <td style="border:none;">
      <iframe width="320" height="180" src="https://www.youtube.com/embed/jzoo5USTUSs?si=f1NjfhT9xxyMb_t5" frameborder="0" allowfullscreen></iframe>
      <div style='text-align:center; font-weight:bold;'>EdgeFool: Adversarial Image Enhancement (ICASSP 2020)</div>
    </td>
    <td style="border:none;"></td>
  </tr>
</table>

## Talks
- 05/2024 - ICLR 2024 conference -- Confidential-DPproof: Confidential Proof of Differentially Private Training <i class="fa fa-youtube-play" style="color:blue"></i>[<font color="blue">Video</font>](https://iclr.cc/virtual/2024/poster/18707)
- 07/2023 - UAI 2023 conference -- Mnemonist: Locating Model Parameters that Memorize Training Examples <i class="fa fa-youtube-play" style="color:blue"></i>[<font color="blue">Video</font>](https://www.youtube.com/watch?v=-GE8BUZzjlo)
- 06/2023 - PETS 2023 conference -- Losing Less: A Loss for Differentially Private Deep Learning <i class="fa fa-file-powerpoint-o" style="color:orange"></i>[<font color="orange">Slides</font>](https://drive.google.com/file/d/1leWqbEArDvZzM_zCEtjk6i_Vk5uvTpnF/view) <i class="fa fa-youtube-play" style="color:blue"></i>[<font color="blue">Video</font>](https://www.youtube.com/watch?v=Eet3AJeqTEY)
- 06/2023 - PETS 2023 conference -- Differentially Private Speaker Anonymization <i class="fa fa-file-powerpoint-o" style="color:orange"></i>[<font color="orange">Slides</font>](https://drive.google.com/file/d/1s-da_nW0Rz-RJO6Rovo0qM8auvpOYTg8/view) <i class="fa fa-youtube-play" style="color:blue"></i>[<font color="blue">Video</font>](https://www.youtube.com/watch?v=uvc49vW-UUI)
- 05/2023 - ICLR 2023 conference -- Confidential-PROFITT: Confidential PROof of FaIr Training of Trees <i class="fa fa-youtube-play" style="color:blue"></i>[<font color="blue">Video</font>](https://iclr.cc/virtual/2023/poster/11224)
- 05/2023 - [Workshop on Algorithmic Audits of Algorithms](https://algorithmic-audits.github.io) 
- 05/2023 - Intel
- 04/2023 - Northwestern University -- How can we audit Fairness of AI-driven services provided by companies?
- 03/2023 - AIUK 2023 -- Confidential-PROFITT: Confidential PROof of FaIr Training of Trees <i class="fa fa-youtube-play" style="color:blue"></i>[<font color="blue">Video</font>](https://www.youtube.com/watch?app=desktop&v=np6-cSckPV4&t=2364s)
- 03/2023 - University of Cambridge -- An Overview of Differential Privacy, Membership Inference Attacks, and Federated Learning 
- 11/2022 - NeurIPS 2022 conference -- Washing The Unwashable : On The (Im)possibility of Fairwashing Detection <i class="fa fa-youtube-play" style="color:blue"></i>[<font color="blue">Video</font>](https://neurips.cc/virtual/2022/poster/54741)
- 11/2022 - University of Cambridge and Samsung  
- 10/2022 - Queen's University of Belfast  
- 09/2022 - Information Commissioner's Office  
- 09/2022 - Brave
- 06/2020 - CVPR 2020 conference -- ColorFool: Semantic Adversarial Colorization <i class="fa fa-youtube-play" style="color:blue"></i>[<font color="blue">Video</font>](https://www.youtube.com/watch?v=fGw1ZiqOrWo)
- 05/2020 - ACM Multimedia 2020 -- A tutorial on Deep Learning for Privacy in Multimedia <i class="fa fa-file-powerpoint-o" style="color:orange"></i>[<font color="orange">Slides</font>](https://cis.eecs.qmul.ac.uk/pdfs/2020.10.12__DeepLearningForPrivacyInMultimedia_Part2.pdf)
- 05/2020 - ICASSP 2020 conference -- EdgeFool: An Adversarial Image Enhancement Filter <i class="fa fa-youtube-play" style="color:blue"></i>[<font color="blue">Video</font>](https://www.youtube.com/watch?v=jzoo5USTUSs&t=1s)
- 06/2018 - The Alan Turing Institute -- Privacy-Aware Neural Network Classification & Training -- <i class="fa fa-youtube-play" style="color:blue"></i>[<font color="blue">Video</font>](https://www.youtube.com/watch?v=pJtw6IRo9q4)
- 06/2018 - QMUL summer school -- Distribute One-Class Learning <i class="fa fa-youtube-play" style="color:blue"></i>[<font color="blue">Video</font>](https://www.youtube.com/watch?v=9w_TP8iwpxI)
