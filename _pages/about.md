---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
show_toc: true
redirect_from: 
  - /about/
  - /about.html
---

<p>
  This is Dr Ali Shahin Shamsabadi (<a href="mailto:ashahinshamsabadi@brave.com">ashahinshamsabadi@brave.com</a>)! I am a Senior Research Scientist (now expanding into product strategy and cross-functional leadership) at <a href="https://brave.com">Brave Software</a>. I collaborate across disciplines and organizations to turn scientific insights into innovative, impactful products. Before joining Brave Software, I was a Research Scientist at 
  <a href="https://www.turing.ac.uk">The Alan Turing Institute</a> (Safe and Ethical AI) under the supervision of 
  <a href="http://mlg.eng.cam.ac.uk/adrian/">Adrian Weller</a>, and a Postdoctoral Fellow at 
  <a href="https://vectorinstitute.ai">Vector Institute</a> under the supervision of 
  <a href="https://www.papernot.fr">Nicolas Papernot</a>. During my PhD, I was very fortunate to work under 
  <a href="http://researchers.lille.inria.fr/abellet/">Aur√©lien Bellet</a>, 
  <a href="https://scholar.google.com/citations?hl=en&user=KZmcljoAAAAJ&view_op=list_works&sortby=pubdate">Andrea Cavallaro</a>, 
  <a href="https://scholar.google.com/citations?hl=en&user=M1FQ49sAAAAJ&view_op=list_works&sortby=pubdate">Adria Gascon</a>, 
  <a href="https://www.imperial.ac.uk/people/h.haddadi">Hamed Haddadi</a>, 
  <a href="https://mkusner.github.io">Matt Kusner</a> and 
  <a href="https://members.loria.fr/EVincent/">Emmanuel Vincent</a>.
</p>
<p>
  My research has been cited in the press including 
  <a href="https://www.thurrott.com/a-i/298422/brave-proposes-a-framework-to-ensure-machine-learning-models-are-trained-privately">
    Thurrott
  </a> and 
  <a href="https://www.turing.ac.uk/sites/default/files/2023-06/pioneering_new_approaches_to_verifying_the_fairness_of_ai_models_0.pdf">
    TuringTop10
  </a>.
</p>

<div class="research-section">
<h2 id="blog-posts">Blog Posts and Press</h2>

<ul>
  <li><a href="https://brave.com/blog/privacy-in-llms/">Membership Privacy Risks in LLMs</a></li>
  <li><a href="https://brave.com/blog/nebula/">Differentially private data collection</a></li>
  <li><a href="https://brave.com/confidential-dpproof/">Verifiable differentially private ML</a></li>
  <li><a href="https://medium.com/@ashahin.ee/zero-knowledge-proofs-made-easy-understanding-and-applying-zero-knowledge-proof-in-machine-ccc3a41853c2">Applying Zero Knowledge Proof in ML</a></li>
  <li><a href="https://www.cleverhans.io/2022/04/17/fl-privacy.html">Privacy risks in Federated Learning</a></li>
</ul>

</div>




<div class="research-section">
<h2 id="research">Research</h2>

My research initiates a fundamental question: 
<strong> How can we reliably verify the trustworthiness of AI-based services, given that: i) AI-based services are provided as "black-boxes" to protect intellectual property; ii) Institutions are materially disincentivized from trustworthy behavior. </strong>

<h3>Verifiable Trustworthiness of AI in Practice</h3>

<div class="publication-list">
<ul>
<li>Verifiable alignment: <a href="https://alishahin.github.io/">SURE: SecUrely REpairs failures flagged by users</a> [NeurIPS'2025]</li>
<li>Verifiable Uncertainty: <a href="https://arxiv.org/pdf/2505.23968">Confidential Guardian</a> [ICML'2025]</li>
<li>Verifiable Privacy: <a href="https://openreview.net/pdf?id=PQY2v6VtGe">Confidential-DPproof</a> [ICLR'2024]</li>
<li>Verifiable Fairness: <a href="https://openreview.net/forum?id=iIfDQVyuFD">Confidential-PROFITT</a> [ICLR'2023], <a href="https://arxiv.org/pdf/2410.02777">OATH</a> [NeurIPS'2025]</li>
<li><a href="https://openreview.net/forum?id=OUz_9TiTv9j">Architecture-Independent Model Distances</a> [ICLR'2022]</li>
</ul>
</div>

<h3>Identifying failure modes for AI systems</h3>

<div class="publication-list">
<ul>
  <li>Privacy Attacks: <a href="https://arxiv.org/pdf/2409.13745">Context-Aware MIAs against Pre-trained LLMs</a> [EMNLP'2025], <a href="https://arxiv.org/pdf/2508.07054">Membership and Memorization in LLM Knowledge Distillation</a> [EMNLP'2025], <a href="https://openreview.net/pdf?id=oVn5GLyONY">Locating Model Parameters that Memorize Training Examples</a> [UAI'2023], <a href="https://arxiv.org/pdf/2112.02918.pdf">Trap weights</a> [Euro S&amp;P'2023]</li>
  <li>Fairness Attacks: <a href="https://openreview.net/pdf?id=3vmKQUctNy">Fairwashing</a> [NeurIPS'2022]</li>
  <li>Robustness Attacks: <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Shamsabadi_ColorFool_Semantic_Adversarial_Colorization_CVPR_2020_paper.pdf">ColorFool</a> [CVPR'2020], <a href="https://arxiv.org/pdf/2202.02751.pdf">Mystique</a> [USENIX'2022], <a href="https://arxiv.org/pdf/1910.12227.pdf">EdgeFool</a> [ICASSP'2020], <a href="https://arxiv.org/pdf/2008.06069.pdf">FilterFool</a> [TIP'2022], and <a href="https://arxiv.org/pdf/2011.08483.pdf">FoolHD</a> [ICASSP'2021]</li>
</ul>
</div>

<h3>Secure and privacy-preserving (by design) AI</h3>

<div class="publication-list">
<ul>
  <li>Privacy-preserving:  
    <a href="https://petsymposium.org/popets/2023/popets-2023-0083.pdf">A Loss for Differentially Private Deep Learning</a> [PETS'2023], 
    <a href="https://arxiv.org/pdf/2202.11823.pdf">Differentially Private Speaker Anonymization</a> [PETS'2023], 
    <a href="https://arxiv.org/pdf/2203.00949.pdf">Differentially Private Graph Neural Networks</a> [USENIX'2022],
    <a href="https://arxiv.org/pdf/1802.03151.pdf">Deep Private-Feature Extraction</a>
  </li>
  <li>Secure:  
    <a href="https://arxiv.org/pdf/1907.03372">Two-Party Secure Neural Network Training and Prediction</a> [CCS'2019], 
    <a href="https://arxiv.org/pdf/2004.05703">Model Privacy at the Edge using Trusted Execution Environments</a> [MobiSys'2020],  
    <a href="https://arxiv.org/pdf/2004.05574">From Local to Distributed Private Training and Prediction</a> [TIFS'2020]
  </li>
</ul>
</div>

</div>


<div class="research-section">
<h2 id="product">Product</h2>

<h3>Privacy Preserving Product Analytics</h3>
<p><a href="https://brave.com/blog/nebula/">Nebula</a>: a novel, practical and best-in-class system for product usage analytics with differential privacy guarantees! Nebula puts users first in product analytics: i) Formal Differential Privacy Protection; ii) Auditability, Verifiability, and Transparency; and iii) Efficiency with Minimal Impact.</p>

<h3>Privacy-Preserving Conversation Analytics</h3>
<p>Coming soon.</p>

<h3>Secure, Privacy-Preserving and Efficient Agents</h3>
<p>Coming soon.</p>

</div> 


<div class="research-section">
<h2 id="recent-students">Recent Students</h2>

<div class="publication-list">
<ul>
<li><a href="https://jrohsc.github.io/">Jaechul Roh</a>: Brave PhD Intern (Summer 2025), Project: Privacy of web agents</li>
<li><a href="https://dzungvpham.github.io/">Dzung V. Pham</a>: Brave PhD Intern (Summer 2025), Project: Efficiency of web agents</li>
<li><a href="https://www.alishaukani.com/">Alisha Ukani</a>: Brave PhD Intern (Summer 2025), Project: Intersection of Browser privacy and Web agents</li>
<li><a href="https://saiid.ch/">Saiid El Hajj Chehade</a>: Brave PhD Visitor (Summer 2025), Project: Optimizing web presentation for agents</li>
<li><a href="https://www.comp.nus.edu.sg/~hongyan/">Hongyan Chang</a>: Brave PhD Intern (Summer 2024), Project: <a href="https://arxiv.org/pdf/2409.13745">Context-Aware Membership Inference Attacks against Pre-trained Large Language Models</a></li>
<li><a href="https://scholar.google.com/citations?hl=en&user=V0918CIAAAAJ&view_op=list_works&sortby=pubdate">Olive Franzese</a>: Brave PhD Intern (Summer 2024), Project: <a href="https://arxiv.org/pdf/2410.02777">OATH: Efficient and Flexible Zero-Knowledge Proofs of End-to-End ML Fairness</a></li>
<li><a href="https://v-smith.github.io/">Victoria Smith</a>: Alan Turing Institute PhD Enrichment Student (Fall 2022 - Fall 2023), Project: <a href="https://arxiv.org/pdf/2310.01424.pdf">Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey</a></li>
</ul>
</div>

</div>


<div class="research-section">
<h2 id="news">News</h2>

<div class="publication-list">
<ul>
<li><strong>[September 2025]</strong> Two papers accepted at The Thirty-Ninth Annual Conference on Neural Information Processing Systems <a href="https://neurips.cc/">NeurIPS 2025</a>, called <a href="https://alishahin.github.io/">Pin the Tail on the Model: Blindfolded Repair of User-Flagged Failures in Text-to-Image Services</a> and <a href="https://alishahin.github.io/">Secure and Confidential Certificates of Online Fairness</a>.</li></li>
<li><strong>[August 2025]</strong> Two papers accepted at the conference on Empirical Methods in Natural Language Processing <a href="https://2025.emnlp.org/">EMNLP 2025</a>, called <a href="https://arxiv.org/abs/2508.07054">Membership and Memorization in LLM Knowledge Distillation</a> and <a href="https://arxiv.org/abs/2409.13745">Context-Aware Membership Inference Attacks against Pre-trained Large Language Models</a>.</li></li>
<li><strong>[May 2025]</strong> Paper accepted at the 32nd ACM Conference on Computer and Communications Security <a href="https://www.sigsac.org/ccs/CCS2025/">CCS2025</a>, called <a href="https://arxiv.org/pdf/2409.09676">Nebula: Efficient, Private and Accurate Histogram Estimation</a>.</li></li>
<li><strong>[April 2025]</strong> Paper accepted at the 42nd International Conference on Machine Learning <a href="https://icml.cc/">ICML2025</a>, called <a href="https://arxiv.org/pdf/2505.23968">Confidential Guardian: Cryptographically Prohibiting the Abuse of Model Abstention</a>.</li></li>
<li><strong>[November 2024]</strong> Gave a guest lecture at Imperial College London: <a href="https://drive.google.com/file/d/1mgXaNLjRpWKS7KQioawO4xiBG_iqYgmE/view?usp=sharing">Collecting Speech and Telemetry Data Privately</a>.</li></li>
<li><strong>[October 2024]</strong> Gave a research talk at Google TechTalk: <a href="https://drive.google.com/file/d/1i8MSiDQ6Uvyvx0H56fqaNf0kNefERfSr/view?usp=sharing">Beyond Trust: Proving Fairness and Privacy in Machine Learning</a>.</li></li>
<li><strong>[September 2024]</strong> A new preprint on LLMs and privacy, called <a href="https://arxiv.org/pdf/2409.13745">Context-Aware Membership Inference Attacks against Pre-trained Large Language Models</a>.</li></li>
<li><strong>[September 2024]</strong> My first-ever privacy-preserving product, <a href="https://brave.com/blog/nebula/">Differentially private data collection</a>.</li></li>
<li><strong>[September 2024]</strong> A new preprint on differential privacy and telemetry data, called <a href="https://arxiv.org/pdf/2409.09676">Nebula: Efficient, Private and Accurate Histogram Estimation</a>.</li></li>
<li><strong>[January 2024]</strong> Paper accepted at the 12th International Conference on Learning Representations <a href="https://iclr.cc/">ICLR2024</a>, called <a href="https://hal.science/hal-04610635/document">Confidential-DPproof: Confidential Proof of Differentially Private Training</a>. <span style="background-color:red"><font color="white"> spotlight </font></span></li></li>
<li><strong>[September 2023]</strong> A new preprint on LLMs and privacy, called <a href="https://arxiv.org/pdf/2310.01424.pdf">Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey</a>.</li>
<li><strong>[July 2023]</strong> Recieved Best Reviewers Free Registration award from <a href="https://fl-icml2023.github.io/">FL@ICML</a>!</li>
<li><strong>[July 2023]</strong> Our project <a href="https://openreview.net/forum?id=iIfDQVyuFD">Confidential-PROFITT: Confidential PROof of FaIr Training of Trees</a> is selected as Turing's top 10 projects of 2022-2023, see <a href="https://t.co/I2b4PUPkbs">Pioneering New Approaches to Verifying the Fairness of AI Models</a>!</li>
<li><strong>[July 2023]</strong> Presented 2 papers at <a href="https://petsymposium.org/2023/">PETS2023</a>: <a href="https://drive.google.com/file/d/1s-da_nW0Rz-RJO6Rovo0qM8auvpOYTg8/view?usp=share_link">Differentially Private Speaker Anonymization</a> and <a href="https://drive.google.com/file/d/1leWqbEArDvZzM_zCEtjk6i_Vk5uvTpnF/view?usp=share_link">Losing Less: A Loss for Differentially Private Deep Learning</a>!</li>
<li><strong>[July 2023]</strong> Started as a Privacy Researcher at <a href="https://brave.com">Brave Software</a>!</li>
<li><strong>[May 2023]</strong> Paper accepted at the 39th Conference on Uncertainty in Artificial Intelligence <a href="https://www.auai.org/uai2023/">UAI</a>, called <a href="https://openreview.net/pdf?id=oVn5GLyONY">Mnemonist: Locating Model Parameters that Memorize Training Examples</a>. </li>
<li><strong>[March 2023]</strong> Paper accepted at the 23rd Privacy Enhancing Technologies symposium <a href="https://petsymposium.org/index.php">PETs</a>, called <a href="https://petsymposium.org/popets/2023/popets-2023-0083.pdf">Losing Less: A Loss for Differentially Private Deep Learning</a>.</li>
<li><strong>[March 2023]</strong> Organising AI UK 2023 workshop, <a href="https://private-fair-ai.github.io">Privacy and Fairness in AI for Health</a>.</li>
<li><strong>[January 2023]</strong> Paper accepted at the 11th International Conference on Learning Representations <a href="https://iclr.cc">ICLR</a>, called <a href="https://openreview.net/forum?id=iIfDQVyuFD">Confidential-PROFITT: Confidential PROof of FaIr Training of Trees</a>. <span style="background-color:red"><font color="white"> notable top 5% of accepted papers </font></span></li>
<li><strong>[November 2022]</strong> 2 papers accepted at the 32nd <a href="https://www.usenix.org/conference/usenixsecurity23">USENIX Security Symposium</a>, called <a href="https://arxiv.org/pdf/2203.00949.pdf">GAP: Differentially Private Graph Neural Networks with Aggregation Perturbation</a> and <a href="https://alishahin.github.io">Tubes Among US: Analog Attack on Automatic Speaker Identification</a>.</li>
<li><strong>[November 2022]</strong> Co-organizing the Privacy Preserving Machine Learning <a href="https://ppml-workshop.github.io/ppml22/">PPML'2022</a> workshop co-located with <a href="https://focs2022.eecs.berkeley.edu">FOCS'2022</a>.</li>
<li><strong>[September 2022]</strong> Paper accepted at the 36th Conference on Neural Information Processing Systems <a href="https://neurips.cc">NeurIPS</a>, called <a href="https://openreview.net/pdf?id=3vmKQUctNy">Washing The Unwashable: On The (Im)possibility of Fairwashing Detection</a>, <a href="https://github.com/cleverhans-lab/FRAUD-Detect"><font color="blue">Code</font></a>.</li>
<li><strong>[September 2022]</strong> 2 papers accepted at the 23rd Privacy Enhancing Technologies symposium <a href="https://petsymposium.org/index.php">PETs</a>, called <a href="https://petsymposium.org/popets/2023/popets-2023-0007.pdf">Differentially Private Speaker Anonymization</a> and <a href="https://alishahin.github.io">Private Multi-Winner Voting for Machine Learning</a>.</li>
<li><strong>[August 2022]</strong> Chair and organise <a href="https://scholar.google.com/citations?hl=en&user=lzfVm_8AAAAJ&view_op=list_works&sortby=pubdate">Olya Ohrimenko</a> in-person talk at Turing AI Programme.</li>
<li><strong>[July 2022]</strong> Chair and organise <a href="https://www.papernot.fr">Nicolas Papernot</a> in-person talk at Turing AI Programme.</li>
<li><strong>[January 2022]</strong> Paper accepted at the 10th International Conference on Learning Representations <a href="https://iclr.cc">ICLR</a>, called <a href="https://openreview.net/forum?id=OUz_9TiTv9j">A Zest of LIME: Towards Architecture-Independent Model Distances</a>, <a href="https://github.com/cleverhans-lab/Zest-Model-Distance"><font color="blue">Code</font></a>.</li>
<li><strong>[December 2021]</strong> A new preprint on federated learning privacy attack, called <a href="https://arxiv.org/pdf/2112.02918.pdf">When the Curious Abandon Honesty: Federated Learning Is Not Private</a>.</li>
<li><strong>[November 2021]</strong> Started as a Research Associate at <a href="https://www.turing.ac.uk">The Alan Turing Institute</a> under supervision of <a href="http://mlg.eng.cam.ac.uk/adrian/">Adrian Weller</a>.</li>
<li><strong>[August 2021]</strong> Paper accepted at IEEE Transactions on Image Processing <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>, called <a href="https://arxiv.org/pdf/2008.06069.pdf">Semantically Adversarial Learnable Filters</a>, <a href="https://github.com/AliShahin/FilterFool"><font color="blue">Code</font></a>!</li>
<li><strong>[March 2021]</strong> Successfully passed PhD viva!</li></li>
<li><strong>[February 2021]</strong> Started a Postdoctoral fellow at <a href="https://vectorinstitute.ai">Vector Institute</a> under supervision of <a href="https://www.papernot.fr">Nicolas Papernot</a>.</li>
<li><strong>[January 2021]</strong> Paper accepted at 46th International Conference on Acoustics, Speech, and Signal Processing, <a href="https://2021.ieeeicassp.org">ICASSP2021</a>, called <a href="https://arxiv.org/pdf/2011.08483.pdf">FoolHD: Fooling speaker identification by Highly imperceptible adversarial Disturbances</a>, <a href="https://fsepteixeira.github.io/FoolHD/"><font color="blue">Code</font></a>! (acceptance rate 19%)</li>
<li><strong>[October 2020]</strong> Giving a <a href="http://cis.eecs.qmul.ac.uk/privacymultimedia.html">toturial</a> at <a href="https://2020.acmmm.org">ACM Multimedia 2020 conference</a>, Part2: adversarial images. </li>
<li><strong>[September 2020]</strong> Offered an internship at <a href="https://www.inria.fr/en/centre-inria-lille-nord-europe">Inria</a>, under supervision of Aurelien Bellet.</li>
<li><strong>[April 2020]</strong> Selected as 200 young researchers from all over the world for 8th <a href="https://www.heidelberg-laureate-forum.org/about-us.html">HEIDELBERG LAUREATE FORUM</a> by international experts appointed by award-granting institutions: The Association for Computing Machinery (ACM), the Norwegian Academy of Science and Letters (DNVA) and the International Mathematical Union (IMU)!</li>
<li><strong>[March 2020]</strong> Paper accepted at IEEE Transactions on Information Forensics and Security <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206">TIFS</a>, called <a href="https://arxiv.org/pdf/2004.05574.pdf">PrivEdge: From Local to Distributed Private Training and Prediction</a>, <a href="https://github.com/smartcameras/PrivEdge"><font color="blue">Code</font></a>! (impact factor 6.2)</li>
<li><strong>[March 2020]</strong> Paper accepted at IEEE Transactions on Multimedia <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">TMM</a>, called <a href="https://arxiv.org/pdf/2007.09766.pdf">Exploiting Vulnerabilities of Deep Neural Networks for Privacy Protection</a>, <a href="https://github.com/smartcameras/RP-FGSM"><font color="blue">Code</font></a>! (impact factor 5.5)</li>
<li><strong>[March 2020]</strong> Paper accepted at ACM International Conference on Mobile Systems, Applications, and Services <a href="https://www.sigmobile.org/mobisys/2020/">MobiSys</a>, called <a href="https://arxiv.org/pdf/2004.05703.pdf">DarkneTZ: Towards Model Privacy on the Edge using Trusted Execution Environments</a>, <a href="https://github.com/mofanv/darknetz"><font color="blue">Code</font></a>! (acceptance rate 19%)</li>
<li><strong>[Feb 2020]</strong> Paper accepted at Conference on Computer Vision and Pattern Recognition, <a href="http://cvpr2020.thecvf.com">CVPR2020</a>, called <a href="https://arxiv.org/pdf/1911.10891.pdf">ColorFool: Semantic Adversarial Colorization</a>, <a href="https://www.youtube.com/watch?v=fGw1ZiqOrWo">Video</a>, <a href="https://github.com/smartcameras/ColorFool"><font color="blue">Code</font></a>! (acceptance rate 22%)</li>
<li><strong>[Jan 2020]</strong> Paper accepted at 45th International Conference on Acoustics, Speech, and Signal Processing, <a href="https://2020.ieeeicassp.org">ICASSP2020</a>, called <a href="https://arxiv.org/pdf/1910.12227.pdf">EDGEFOOL: AN ADVERSARIAL IMAGE ENHANCEMENT FILTER</a>, <a href="https://www.youtube.com/watch?time_continue=16&v=jzoo5USTUSs&feature=emb_logo">Video</a>, <a href="https://github.com/smartcameras/EdgeFool"><font color="blue">Code</font></a>! (acceptance rate 19%)</li>
<li><strong>[Jan 2020]</strong> Paper accepted at IEEE Internet of Things Journal called <a href="https://arxiv.org/pdf/1703.02952.pdf">A Hybrid Deep Learning Architecture for Privacy-Preserving Mobile Analytics</a>! (impact factor 9.5)</li>
<li><strong>[April 2019]</strong> Paper accepted at 26th ACM Conference on Computer and Communications Security, <a href="https://sigsac.org/ccs/CCS2019/">CCS2019</a>, called <a href="https://arxiv.org/pdf/1907.03372.pdf">QUOTIENT: Two-Party Secure Neural Network Training and Prediction</a>! (acceptance rate 16%)</li>
<li><strong>[Jan 2019]</strong> Paper accepted at 44th International Conference on Acoustics, Speech, and Signal Processing, <a href="https://2019.ieeeicassp.org">ICASSP2019</a>, called <a href="https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/56780/Cavallaro%20Scene%20privacy%20protection%202019%20Accepted.pdf?sequence=2">Scene privacy protection</a>, <a href="https://github.com/smartcameras/P-FGSM"><font color="blue">Code</font></a>! (acceptance rate 49%)</li>
<li><strong>[June 2018]</strong> Offered a PhD Enrichment scheme, a 9-month placement at <a href="https://www.turing.ac.uk">The Alan Turing Institute</a>!</li>
<li><strong>[March 2018]</strong> Offered an internship for summer 2018 at <a href="https://www.turing.ac.uk">The Alan Turing Institute</a>, working on project <a href="https://aticdn.s3-eu-west-1.amazonaws.com/2017/12/Internship-Project-Descriptions-2018-FINAL-WC.pdf">Privacy-aware neural network classification & training</a> under supervision of Adria Gascon, Matt Kusner, Varun Kanade!</li>
</ul>
</div>

</div>

<div class="research-section">
<h2 id="pc-services">PC services</h2>

<div class="publication-list">
<ul>
<li>2024: PETs'2024, ICLR'2024, <a href="https://pml-workshop.github.io/iclr24/">ICLR Private ML workshop</a></li>
<li>2023: NeurIPS'2023, CCS'2023, AISTATS'2023, <a href="https://fl-icml2023.github.io">ICML 2023 workshop federated learning</a></li>
<li>2022: ICML'2022, TIFS'2022, TOPS'2022, <a href="https://sites.google.com/view/2022-workshop-explainable-ai/">Explainable AI in Finance</a></li>
<li>2021: ICLR'2021 <a href="https://aisecure-workshop.github.io/aml-iclr2021/committee">Security and Safety in Machine Learning Systems</a>, ICCV'2021 <a href="https://iccv21-adv-workshop.github.io">Adversarial Robustness In the Real World</a></li>
<li>2020: ECCV'2020 <a href="https://eccv20-adv-workshop.github.io">Adversarial Robustness in the Real World</a>, <a href="http://www.ieee-security.org/TC/SP2021/cfpapers.html">42nd IEEE Symposium on Security and Privacy</a></li>
</ul>
</div>

</div>


<div class="research-section">
<h2 id="selected-research-talks">Selected Research Talks</h2>

<table style="border-collapse:collapse; border:none;">
  <tr>
    <td style="border:none;">
      <iframe width="320" height="180" src="https://www.youtube.com/embed/uvc49vW-UUI?si=RLddB3SNoVG7PGyT" frameborder="0" allowfullscreen></iframe>
      <div style='text-align:center; font-weight:bold;'>Differentially Private Speaker Anonymization (PETS 2023)</div>
    </td>
    <td style="border:none;">
      <iframe width="320" height="180" src="https://www.youtube.com/embed/-GE8BUZzjlo?si=7btFNQyMkYN7SSwO" frameborder="0" allowfullscreen></iframe>
      <div style='text-align:center; font-weight:bold;'>Mnemonist: Locating Model Parameters (UAI 2023)</div>
    </td>
    <td style="border:none;">
      <iframe width="320" height="180" src="https://www.youtube.com/embed/Eet3AJeqTEY?si=J98i0-1JG7DjB8_u" frameborder="0" allowfullscreen></iframe>
      <div style='text-align:center; font-weight:bold;'>Losing Less: A Loss for DP Deep Learning (PETS 2023)</div>
    </td>
  </tr>
  <tr>
    <td style="border:none;">
      <iframe width="320" height="180" src="https://www.youtube.com/embed/fGw1ZiqOrWo?si=pTeOkyX3vPpuGr1F" frameborder="0" allowfullscreen></iframe>
      <div style='text-align:center; font-weight:bold;'>ColorFool: Semantic Adversarial Colorization (CVPR 2020)</div>
    </td>
    <td style="border:none;">
      <iframe width="320" height="180" src="https://www.youtube.com/embed/jzoo5USTUSs?si=f1NjfhT9xxyMb_t5" frameborder="0" allowfullscreen></iframe>
      <div style='text-align:center; font-weight:bold;'>EdgeFool: Adversarial Image Enhancement (ICASSP 2020)</div>
    </td>
    <td style="border:none;"></td>
  </tr>
</table>

</div>

<div class="research-section">
<h2 id="talks">Talks</h2>

<div class="publication-list">
<ul>
<li>05/2024 - ICLR 2024 conference -- Confidential-DPproof: Confidential Proof of Differentially Private Training <i class="fa fa-youtube-play" style="color:blue"></i><a href="https://iclr.cc/virtual/2024/poster/18707"><font color="blue">Video</font></a></li>
<li>07/2023 - UAI 2023 conference -- Mnemonist: Locating Model Parameters that Memorize Training Examples <i class="fa fa-youtube-play" style="color:blue"></i><a href="https://www.youtube.com/watch?v=-GE8BUZzjlo"><font color="blue">Video</font></a></li>
<li>06/2023 - PETS 2023 conference -- Losing Less: A Loss for Differentially Private Deep Learning <i class="fa fa-file-powerpoint-o" style="color:orange"></i><a href="https://drive.google.com/file/d/1leWqbEArDvZzM_zCEtjk6i_Vk5uvTpnF/view"><font color="orange">Slides</font></a> <i class="fa fa-youtube-play" style="color:blue"></i><a href="https://www.youtube.com/watch?v=Eet3AJeqTEY"><font color="blue">Video</font></a></li>
<li>06/2023 - PETS 2023 conference -- Differentially Private Speaker Anonymization <i class="fa fa-file-powerpoint-o" style="color:orange"></i><a href="https://drive.google.com/file/d/1s-da_nW0Rz-RJO6Rovo0qM8auvpOYTg8/view"><font color="orange">Slides</font></a> <i class="fa fa-youtube-play" style="color:blue"></i><a href="https://www.youtube.com/watch?v=uvc49vW-UUI"><font color="blue">Video</font></a></li>
<li>05/2023 - ICLR 2023 conference -- Confidential-PROFITT: Confidential PROof of FaIr Training of Trees <i class="fa fa-youtube-play" style="color:blue"></i><a href="https://iclr.cc/virtual/2023/poster/11224"><font color="blue">Video</font></a></li>
<li>05/2023 - <a href="https://algorithmic-audits.github.io">Workshop on Algorithmic Audits of Algorithms</a></li>
<li>05/2023 - Intel</li>
<li>04/2023 - Northwestern University -- How can we audit Fairness of AI-driven services provided by companies?</li>
<li>03/2023 - AIUK 2023 -- Confidential-PROFITT: Confidential PROof of FaIr Training of Trees <i class="fa fa-youtube-play" style="color:blue"></i><a href="https://www.youtube.com/watch?app=desktop&v=np6-cSckPV4&t=2364s"><font color="blue">Video</font></a></li>
<li>03/2023 - University of Cambridge -- An Overview of Differential Privacy, Membership Inference Attacks, and Federated Learning</li>
<li>11/2022 - NeurIPS 2022 conference -- Washing The Unwashable : On The (Im)possibility of Fairwashing Detection <i class="fa fa-youtube-play" style="color:blue"></i><a href="https://neurips.cc/virtual/2022/poster/54741"><font color="blue">Video</font></a></li>
<li>11/2022 - University of Cambridge and Samsung</li>
<li>10/2022 - Queen's University of Belfast</li>
<li>09/2022 - Information Commissioner's Office</li>
<li>09/2022 - Brave</li>
<li>06/2020 - CVPR 2020 conference -- ColorFool: Semantic Adversarial Colorization <i class="fa fa-youtube-play" style="color:blue"></i><a href="https://www.youtube.com/watch?v=fGw1ZiqOrWo"><font color="blue">Video</font></a></li>
<li>05/2020 - ACM Multimedia 2020 -- A tutorial on Deep Learning for Privacy in Multimedia <i class="fa fa-file-powerpoint-o" style="color:orange"></i><a href="https://cis.eecs.qmul.ac.uk/pdfs/2020.10.12__DeepLearningForPrivacyInMultimedia_Part2.pdf"><font color="orange">Slides</font></a></li>
<li>05/2020 - ICASSP 2020 conference -- EdgeFool: An Adversarial Image Enhancement Filter <i class="fa fa-youtube-play" style="color:blue"></i><a href="https://www.youtube.com/watch?v=jzoo5USTUSs&t=1s"><font color="blue">Video</font></a></li>
<li>06/2018 - The Alan Turing Institute -- Privacy-Aware Neural Network Classification & Training -- <i class="fa fa-youtube-play" style="color:blue"></i><a href="https://www.youtube.com/watch?v=pJtw6IRo9q4"><font color="blue">Video</font></a></li>
<li>06/2018 - QMUL summer school -- Distribute One-Class Learning <i class="fa fa-youtube-play" style="color:blue"></i><a href="https://www.youtube.com/watch?v=9w_TP8iwpxI"><font color="blue">Video</font></a></li>
</ul>
</div>

</div>
